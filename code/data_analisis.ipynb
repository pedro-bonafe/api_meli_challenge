{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc09921a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import unicodedata\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721ed62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID             Full Name\n",
      "0   1         María Sánchez\n",
      "1   2          Marta Alonso\n",
      "2   3       Javier González\n",
      "3   4    Carmen López López\n",
      "4   5  Isabel Moreno Moreno\n",
      "(5000, 2)\n",
      "Nombres únicos: 3016\n",
      "count    5000.000000\n",
      "mean       17.314000\n",
      "std         4.644967\n",
      "min         8.000000\n",
      "25%        13.000000\n",
      "50%        17.000000\n",
      "75%        21.000000\n",
      "max        33.000000\n",
      "Name: Full Name, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(\"names_dataset.csv\")\n",
    "\n",
    "# Estructura\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "\n",
    "# Unicidad\n",
    "print(\"Nombres únicos:\", df['Full Name'].nunique())\n",
    "\n",
    "# Longitud de strings\n",
    "print(df['Full Name'].str.len().describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140fcb88",
   "metadata": {},
   "source": [
    "El dataset provisto (names_dataset.csv) contiene:\n",
    "\n",
    "* 5.000 registros\n",
    "\n",
    "* 2 columnas: ID y Full Name\n",
    "\n",
    "* Nombres completos en español, con:\n",
    "\n",
    "* Acentos\n",
    "\n",
    "* Nombres y apellidos compuestos\n",
    "\n",
    "* Cantidad variable de tokens\n",
    "\n",
    "* Ausencia de estructura explícita (nombre / apellido)\n",
    "\n",
    "Por lo tanto, el problema corresponde a matching textual, no a análisis semántico.\n",
    "\n",
    "En primer lugar comprobamos si es conveniente realizar una separación previa de los datos en nombre y apellido dado que comunmente los sistemas de información tienen esta estructura. Además la ponderación mayor al apellido podría ayudar a enfocar más el algoritmo, sin embargo dado que no hay una estructura confiable (el dataset no viene separado previamente ni con un separador) separar nombre y apellido genera un peor score. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68e3ef9",
   "metadata": {},
   "source": [
    "# Alternativas evaluadas\n",
    "## A. Comparación sobre el nombre completo\n",
    "\n",
    "Este enfoque consiste en trabajar directamente sobre el nombre completo tal como aparece en el dataset. Previamente, los textos se normalizan (conversión a minúsculas y eliminación de acentos) para evitar diferencias superficiales.\n",
    "La comparación se realiza sobre el string completo utilizando técnicas de similaridad basadas en tokens y distancia de edición, lo que permite capturar variaciones de orden, errores de tipeo y omisiones parciales sin asumir una estructura fija del nombre.\n",
    "\n",
    "## B. Separación heurística de nombre y apellido\n",
    "\n",
    "En esta alternativa se intenta dividir automáticamente el nombre completo en dos partes: nombre y apellido, asumiendo que el último token corresponde al apellido y el resto al nombre.\n",
    "Luego, se calcula la similitud de cada componente por separado y se obtiene un score final ponderado, asignando mayor peso al apellido. Este enfoque busca priorizar coincidencias de apellido, aunque depende fuertemente de una heurística que no siempre se cumple en nombres reales.\n",
    "\n",
    "# Experimento empírico\n",
    "\n",
    "Se realizaron pruebas con consultas realistas contra el dataset, comparando los resultados obtenidos con ambos enfoques. El objetivo fue evaluar no solo el score numérico, sino también la calidad del ranking y la coherencia de las coincidencias devueltas.\n",
    "\n",
    "# Resultados observados\n",
    "\n",
    "El enfoque basado en el nombre completo mostró resultados más precisos y consistentes en la mayoría de los casos.\n",
    "Por el contrario, la separación heurística introdujo varios problemas: aumentó la cantidad de falsos positivos, sobreponderó coincidencias parciales de apellido y perdió información relevante en nombres compuestos. En múltiples escenarios, esto provocó que el resultado correcto quedara peor posicionado en el ranking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e59fbc",
   "metadata": {},
   "source": [
    "| Query             | Mejor match (Full) | Score    | Mejor match (Split)      | Score    |\n",
    "| ----------------- | ------------------ | -------- | ------------------------ | -------- |\n",
    "| Maria Sanchez     | María Sánchez      | 100      | María Sánchez            | 100      |\n",
    "| Juan Carlos Perez | Juan Pérez         | 100      | Juan Pérez               | 100      |\n",
    "| Luis Gonzalez     | Luis González      | 100      | **Luis Martín González** | 100      |\n",
    "| Ana Lopez         | Ana Sánchez López  | 100      | **Ana Jiménez López**    | 100      |\n",
    "| Pedro Ramirez     | Pedro Ruiz         | **78.2** | Pedro Álvarez            | **74.3** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a29344f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rapidfuzz in c:\\users\\pbonafe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.14.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.3 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\pbonafe\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed20671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar dataset\n",
    "df = pd.read_csv(\"names_dataset.csv\")\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    text = \"\".join(c for c in text if not unicodedata.combining(c))\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "df[\"normalized_name\"] = df[\"Full Name\"].apply(normalize)\n",
    "\n",
    "#enfoque A\n",
    "def similarity_full(query: str, candidate: str) -> float:\n",
    "    return fuzz.token_set_ratio(query, candidate)\n",
    "\n",
    "#enfoque B\n",
    "def split_name(full_name: str):\n",
    "    tokens = full_name.split()\n",
    "    if len(tokens) == 1:\n",
    "        return tokens[0], \"\"\n",
    "    return \" \".join(tokens[:-1]), tokens[-1]\n",
    "\n",
    "def similarity_split(query: str, candidate: str, w_name=0.4, w_surname=0.6) -> float:\n",
    "    q_name, q_surname = split_name(query)\n",
    "    c_name, c_surname = split_name(candidate)\n",
    "\n",
    "    score_name = fuzz.token_set_ratio(q_name, c_name) if q_name and c_name else 0\n",
    "    score_surname = fuzz.token_set_ratio(q_surname, c_surname) if q_surname and c_surname else 0\n",
    "\n",
    "    return score_name * w_name + score_surname * w_surname\n",
    "\n",
    "\n",
    "#prueba con datos sintéticos\n",
    "queries = [\n",
    "    \"Maria Sanchez\",\n",
    "    \"Juan Carlos Perez\",\n",
    "    \"Luis Gonzalez\",\n",
    "    \"Ana Lopez\",\n",
    "    \"Pedro Ramirez\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for q in queries:\n",
    "    q_norm = normalize(q)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        full_score = similarity_full(q_norm, row[\"normalized_name\"])\n",
    "        split_score = similarity_split(q_norm, row[\"normalized_name\"])\n",
    "\n",
    "        results.append({\n",
    "            \"query\": q,\n",
    "            \"id\": row[\"ID\"],\n",
    "            \"candidate\": row[\"Full Name\"],\n",
    "            \"full_score\": round(full_score, 2),\n",
    "            \"split_score\": round(split_score, 2)\n",
    "        })\n",
    "\n",
    "#Tabla comparativa final\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d04e7cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>candidate_full</th>\n",
       "      <th>full_score</th>\n",
       "      <th>candidate_split</th>\n",
       "      <th>split_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ana Lopez</td>\n",
       "      <td>Ana López López</td>\n",
       "      <td>100.00</td>\n",
       "      <td>Ana López López</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Juan Carlos Perez</td>\n",
       "      <td>Carlos Pérez</td>\n",
       "      <td>100.00</td>\n",
       "      <td>Carlos Pérez</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Luis Gonzalez</td>\n",
       "      <td>Dr. Luis González</td>\n",
       "      <td>100.00</td>\n",
       "      <td>Dr. Luis González</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Maria Sanchez</td>\n",
       "      <td>María Sánchez</td>\n",
       "      <td>100.00</td>\n",
       "      <td>María Sánchez</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pedro Ramirez</td>\n",
       "      <td>Pedro Ruiz</td>\n",
       "      <td>78.26</td>\n",
       "      <td>Pedro Romero Álvarez</td>\n",
       "      <td>74.29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               query     candidate_full  full_score       candidate_split  \\\n",
       "0          Ana Lopez    Ana López López      100.00       Ana López López   \n",
       "1  Juan Carlos Perez       Carlos Pérez      100.00          Carlos Pérez   \n",
       "2      Luis Gonzalez  Dr. Luis González      100.00     Dr. Luis González   \n",
       "3      Maria Sanchez      María Sánchez      100.00         María Sánchez   \n",
       "4      Pedro Ramirez         Pedro Ruiz       78.26  Pedro Romero Álvarez   \n",
       "\n",
       "   split_score  \n",
       "0       100.00  \n",
       "1       100.00  \n",
       "2       100.00  \n",
       "3       100.00  \n",
       "4        74.29  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "required_cols = {\"query\", \"candidate\", \"full_score\", \"split_score\"}\n",
    "missing = required_cols - set(results_df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"results_df no tiene las columnas requeridas: {sorted(missing)}\")\n",
    "\n",
    "# 1) Mejor match por enfoque FULL (máximo full_score por query; desempate estable)\n",
    "best_full = (\n",
    "    results_df\n",
    "    .sort_values([\"query\", \"full_score\", \"id\"], ascending=[True, False, True], kind=\"mergesort\")\n",
    "    .drop_duplicates(subset=[\"query\"], keep=\"first\")\n",
    "    .rename(columns={\"candidate\": \"candidate_full\", \"full_score\": \"full_score\"})\n",
    "    .loc[:, [\"query\", \"candidate_full\", \"full_score\"]]\n",
    ")\n",
    "\n",
    "# 2) Mejor match por enfoque SPLIT (máximo split_score por query; desempate estable)\n",
    "best_split = (\n",
    "    results_df\n",
    "    .sort_values([\"query\", \"split_score\", \"id\"], ascending=[True, False, True], kind=\"mergesort\")\n",
    "    .drop_duplicates(subset=[\"query\"], keep=\"first\")\n",
    "    .rename(columns={\"candidate\": \"candidate_split\", \"split_score\": \"split_score\"})\n",
    "    .loc[:, [\"query\", \"candidate_split\", \"split_score\"]]\n",
    ")\n",
    "\n",
    "# 3) Tabla comparativa final (lado a lado)\n",
    "comparison = best_full.merge(best_split, on=\"query\", how=\"inner\")\n",
    "\n",
    "comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ac4385",
   "metadata": {},
   "source": [
    "# Conclusión\n",
    "\n",
    "Separar nombre y apellido sin contar con una estructura confiable en los datos introduce heurísticas frágiles que terminan degradando la precisión del matching.\n",
    "En este dataset en particular, no existe una garantía semántica de que el último token represente correctamente el apellido ni de que los tokens restantes correspondan al nombre. Asumir esa estructura genera errores sistemáticos difíciles de corregir.\n",
    "\n",
    "El enfoque más robusto, explicable y defendible consiste en trabajar sobre el nombre completo, aplicando tokenización y técnicas de comparación textual tolerantes a errores, sin forzar divisiones artificiales.\n",
    "\n",
    "# Estrategia final adoptada\n",
    "\n",
    "La estrategia elegida evita la creación de campos artificiales de nombre y apellido y se basa en la normalización del texto y el uso de métricas de similaridad que combinan comparación por tokens y distancia de edición.\n",
    "Los resultados se filtran según un umbral configurable y se ordenan de mayor a menor similitud.\n",
    "\n",
    "Este enfoque reduce falsos positivos, no depende de reglas arbitrarias, es fácil de explicar en una entrevista técnica y escala adecuadamente para el tamaño del dataset analizado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66935d4",
   "metadata": {},
   "source": [
    "# Data mining & Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcdd789",
   "metadata": {},
   "source": [
    "Analizamos el dataset con múltiples ciclos de LLMs: GPT - Gemini para identificar los patrones subyacentes en los datos y poder estandarizar la base de consulta. De esta manera la api puede trabajar sobre datos normalizados. Luego probamos la performance del algoritmo que usamos en la api matcheando la base original vs la base estandarizada para definir cual utilizar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3739b21e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d94fe02",
   "metadata": {},
   "source": [
    "# Análisis exploratorio del dataset de nombres\n",
    "\n",
    "Archivo: `names_dataset.csv`\n",
    "\n",
    "Objetivo: entender **calidad**, **duplicados**, **variantes por errores humanos** y definir una **estandarización**  para usar como base de consulta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0aceb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Full Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>María Sánchez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Marta Alonso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Javier González</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Carmen López López</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Isabel Moreno Moreno</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID             Full Name\n",
       "0   1         María Sánchez\n",
       "1   2          Marta Alonso\n",
       "2   3       Javier González\n",
       "3   4    Carmen López López\n",
       "4   5  Isabel Moreno Moreno"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5000, 2)\n",
      "columns: ['ID', 'Full Name']\n"
     ]
    }
   ],
   "source": [
    "CSV_PATH = \"names_dataset.csv\"  # ajustá si lo movés\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "display(df.head())\n",
    "print(\"shape:\", df.shape)\n",
    "print(\"columns:\", list(df.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dccea2a",
   "metadata": {},
   "source": [
    "## 1) Chequeos básicos (IDs, nulos, duplicados exactos)\n",
    "\n",
    "- Verificamos si `ID` es único.\n",
    "- Cuántos nombres son nulos.\n",
    "- Cuántos duplicados exactos hay en `Full Name`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a5ac541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rows': 5000,\n",
       " 'unique_id': 5000,\n",
       " 'null_full_name': 0,\n",
       " 'unique_full_name_raw': 3016,\n",
       " 'exact_duplicates_raw': 1984}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calidad básica\n",
    "basic = {\n",
    "    \"rows\": len(df),\n",
    "    \"unique_id\": df[\"ID\"].nunique(),\n",
    "    \"null_full_name\": int(df[\"Full Name\"].isna().sum()),\n",
    "    \"unique_full_name_raw\": df[\"Full Name\"].astype(str).nunique(),\n",
    "    \"exact_duplicates_raw\": int(len(df) - df[\"Full Name\"].astype(str).nunique()),\n",
    "}\n",
    "basic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde68ed8",
   "metadata": {},
   "source": [
    "## 2) Distribución de longitudes y cantidad de tokens\n",
    "\n",
    "Nos ayuda a entender si hay nombres compuestos, dobles apellidos, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f61149a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'len_p50': 17.0,\n",
       " 'len_p95': 25.0,\n",
       " 'tokens_distribution': {2: 2228, 3: 2550, 4: 222}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"full_name_raw\"] = df[\"Full Name\"].astype(str)\n",
    "df[\"len_chars\"] = df[\"full_name_raw\"].str.len()\n",
    "df[\"n_tokens_raw\"] = df[\"full_name_raw\"].str.strip().str.split().map(len)\n",
    "\n",
    "stats = {\n",
    "    \"len_p50\": float(df[\"len_chars\"].median()),\n",
    "    \"len_p95\": float(df[\"len_chars\"].quantile(0.95)),\n",
    "    \"tokens_distribution\": df[\"n_tokens_raw\"].value_counts().sort_index().to_dict(),\n",
    "}\n",
    "stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e55052",
   "metadata": {},
   "source": [
    "## 3) Detección de caracteres sospechosos / ruido\n",
    "\n",
    "Buscamos:\n",
    "- Títulos (`Dr.`, `Lic.`, etc.)\n",
    "- Puntuación / símbolos (`$`, `+`, paréntesis)\n",
    "- Doble espacio / espacios al inicio o final\n",
    "\n",
    "Esto suele ser típico de *data entry* humano.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "170ebdcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rows_with_suspicious_chars': 123,\n",
       " 'rows_with_double_space': 136,\n",
       " 'rows_with_leading_or_trailing_space': 0,\n",
       " 'rows_with_title_prefix': 487,\n",
       " 'top_title_prefixes': {'Lic.': 93,\n",
       "  'Col.': 89,\n",
       "  'Dr.': 87,\n",
       "  'Sr.': 80,\n",
       "  'Sra.': 75,\n",
       "  'Mg.': 63}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# caracteres fuera de letras/espacios/'/-\n",
    "pattern_susp = re.compile(r\"[^A-Za-zÁÉÍÓÚÜÑáéíóúüñ\\s'\\-\\.]\")\n",
    "susp = df[df[\"full_name_raw\"].str.contains(pattern_susp, regex=True, na=False)]\n",
    "\n",
    "# espacios\n",
    "df[\"has_double_space\"] = df[\"full_name_raw\"].str.contains(r\"\\s{2,}\", regex=True, na=False)\n",
    "df[\"has_leading_trailing_space\"] = df[\"Full Name\"].astype(str).str.match(r\"^\\s|\\s$\", na=False)\n",
    "\n",
    "# títulos (token inicial terminando en punto)\n",
    "df[\"first_token\"] = df[\"full_name_raw\"].str.strip().str.split().str[0]\n",
    "df[\"has_title_prefix\"] = df[\"first_token\"].str.endswith(\".\", na=False)\n",
    "\n",
    "report_noise = {\n",
    "    \"rows_with_suspicious_chars\": int(len(susp)),\n",
    "    \"rows_with_double_space\": int(df[\"has_double_space\"].sum()),\n",
    "    \"rows_with_leading_or_trailing_space\": int(df[\"has_leading_trailing_space\"].sum()),\n",
    "    \"rows_with_title_prefix\": int(df[\"has_title_prefix\"].sum()),\n",
    "    \"top_title_prefixes\": df[df[\"has_title_prefix\"]][\"first_token\"].value_counts().head(15).to_dict(),\n",
    "}\n",
    "report_noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6304e5ae",
   "metadata": {},
   "source": [
    "aparecen muchos nombres con caracteres como títulos que harán fallar luego el algoritmo de matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a797d44",
   "metadata": {},
   "source": [
    "## 4) Normalización base (acentos, mayúsculas, espacios)\n",
    "\n",
    "Definimos una normalización *suave* (no elimina títulos) para medir cuánto baja la cardinalidad por diferencias superficiales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8ad522e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unique_soft': 2872, 'duplicates_soft': 2128}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def strip_accents(s: str) -> str:\n",
    "    return \"\".join(\n",
    "        c for c in unicodedata.normalize(\"NFKD\", s)\n",
    "        if not unicodedata.combining(c)\n",
    "    )\n",
    "\n",
    "def normalize_soft(raw: str) -> str:\n",
    "    if raw is None or (isinstance(raw, float) and np.isnan(raw)):\n",
    "        return \"\"\n",
    "    s = str(raw).strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = strip_accents(s).lower()\n",
    "    # mantenemos letras, espacios, apóstrofe, guion; removemos el resto\n",
    "    s = re.sub(r\"[^a-z\\s'\\-]\", \"\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "df[\"name_norm_soft\"] = df[\"full_name_raw\"].map(normalize_soft)\n",
    "\n",
    "soft_stats = {\n",
    "    \"unique_soft\": int(df[\"name_norm_soft\"].nunique()),\n",
    "    \"duplicates_soft\": int(len(df) - df[\"name_norm_soft\"].nunique()),\n",
    "}\n",
    "soft_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e09c7a",
   "metadata": {},
   "source": [
    "## 5) Normalización estricta (remueve títulos comunes + símbolos)\n",
    "\n",
    "Como el dataset muestra títulos y símbolos, definimos una normalización *estricta* que:\n",
    "- elimina prefijos tipo `Dr.` / `Lic.` / `Sr.` / `Sra.` / `Mg.` / `Col.`\n",
    "- elimina dígitos y símbolos\n",
    "- mantiene `'` y `-` por apellidos compuestos\n",
    "\n",
    "Esto nos da una base estandarizada más robusta para indexar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3e91a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unique_strict': 2619, 'duplicates_strict': 2381}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TITLE_PREFIXES = {\"dr\", \"lic\", \"sr\", \"sra\", \"mg\", \"col\"}\n",
    "\n",
    "def normalize_strict(raw: str) -> str:\n",
    "    if raw is None or (isinstance(raw, float) and np.isnan(raw)):\n",
    "        return \"\"\n",
    "    s = str(raw).strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "\n",
    "    tokens = s.split(\" \")\n",
    "    if tokens and tokens[0].rstrip(\".\").lower() in TITLE_PREFIXES:\n",
    "        tokens = tokens[1:]\n",
    "    s = \" \".join(tokens)\n",
    "\n",
    "    s = strip_accents(s).lower()\n",
    "    # reemplazamos separadores raros por espacio\n",
    "    s = re.sub(r\"[^\\w\\s'\\-]\", \" \", s)\n",
    "    # removemos dígitos / underscores\n",
    "    s = re.sub(r\"[_\\d]+\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "df[\"name_norm_strict\"] = df[\"full_name_raw\"].map(normalize_strict)\n",
    "\n",
    "strict_stats = {\n",
    "    \"unique_strict\": int(df[\"name_norm_strict\"].nunique()),\n",
    "    \"duplicates_strict\": int(len(df) - df[\"name_norm_strict\"].nunique()),\n",
    "}\n",
    "strict_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145c8b56",
   "metadata": {},
   "source": [
    "## 6) Variantes por mismo nombre normalizado\n",
    "\n",
    "Mide cuántas representaciones distintas (raw) terminan en el mismo valor normalizado (indicador de errores humanos / variantes ortográficas).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbc112fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(292,\n",
       " name_norm_strict\n",
       " javier jimenez      5\n",
       " jose moreno         5\n",
       " pablo jimenez       5\n",
       " sofia martinez      4\n",
       " marta moreno        4\n",
       " marta gonzalez      4\n",
       " marta ruiz          4\n",
       " carlos ruiz         4\n",
       " juan fernandez      4\n",
       " lucia gutierrez     4\n",
       " sofia alonso        4\n",
       " pilar diaz          4\n",
       " javier gonzalez     4\n",
       " sofia alvarez       4\n",
       " carlos rodriguez    4\n",
       " marta hernandez     4\n",
       " sofia hernandez     4\n",
       " sofia gomez         4\n",
       " lucia ruiz          4\n",
       " jose rodriguez      4\n",
       " Name: full_name_raw, dtype: int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variants = (\n",
    "    df.groupby(\"name_norm_strict\")[\"full_name_raw\"]\n",
    "      .nunique()\n",
    "      .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "# cuántos normalizados tienen más de 1 variante raw\n",
    "multi_variant_count = int((variants > 1).sum())\n",
    "\n",
    "# ejemplos\n",
    "examples = variants[variants > 1].head(20)\n",
    "\n",
    "multi_variant_count, examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b762323",
   "metadata": {},
   "source": [
    "## 7) Duplicados frecuentes\n",
    "\n",
    "Top nombres más repetidos (raw vs normalizado). Sirve para entender si hay muchos nombres comunes y qué impacto tiene en el matching.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acab0d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count_raw</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full_name_raw</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Marta Sánchez</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pilar Sánchez</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fernando Rodríguez</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>María Martínez</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sofía Martín</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fernando Gutiérrez</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>María Jiménez</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Javier Muñoz</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Francisco Alonso</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Juan Romero</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pablo Martín</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Francisco Gutiérrez</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carmen González</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Javier Fernández</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sofía Ruiz</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     count_raw\n",
       "full_name_raw                 \n",
       "Marta Sánchez               12\n",
       "Pilar Sánchez               12\n",
       "Fernando Rodríguez          11\n",
       "María Martínez              11\n",
       "Sofía Martín                11\n",
       "Fernando Gutiérrez          11\n",
       "María Jiménez               11\n",
       "Javier Muñoz                10\n",
       "Francisco Alonso            10\n",
       "Juan Romero                 10\n",
       "Pablo Martín                10\n",
       "Francisco Gutiérrez         10\n",
       "Carmen González              9\n",
       "Javier Fernández             9\n",
       "Sofía Ruiz                   9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count_norm_strict</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name_norm_strict</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>marta sanchez</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jose moreno</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>juan romero</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sofia martin</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sofia alonso</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pablo martin</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pilar sanchez</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>francisco gutierrez</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maria jimenez</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pedro gutierrez</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fernando gutierrez</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maria martinez</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fernando rodriguez</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lucia gutierrez</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rafael rodriguez</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     count_norm_strict\n",
       "name_norm_strict                      \n",
       "marta sanchez                       13\n",
       "jose moreno                         13\n",
       "juan romero                         12\n",
       "sofia martin                        12\n",
       "sofia alonso                        12\n",
       "pablo martin                        12\n",
       "pilar sanchez                       12\n",
       "francisco gutierrez                 11\n",
       "maria jimenez                       11\n",
       "pedro gutierrez                     11\n",
       "fernando gutierrez                  11\n",
       "maria martinez                      11\n",
       "fernando rodriguez                  11\n",
       "lucia gutierrez                     11\n",
       "rafael rodriguez                    10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_raw = df[\"full_name_raw\"].value_counts().head(15)\n",
    "top_strict = df[\"name_norm_strict\"].value_counts().head(15)\n",
    "\n",
    "display(top_raw.to_frame(\"count_raw\"))\n",
    "display(top_strict.to_frame(\"count_norm_strict\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a3eb0e",
   "metadata": {},
   "source": [
    "## 8) Hallazgos y decisiones (para el informe)\n",
    "\n",
    "### Hallazgos principales\n",
    "- El dataset contiene ruido típico de carga humana (títulos con punto, símbolos y puntuación).\n",
    "- Hay duplicados exactos y duplicados que aparecen solo después de normalizar (acentos/mayúsculas/espacios/puntuación).\n",
    "- No existe estructura confiable para separar nombre/apellido (nombres compuestos y cantidad de tokens variable).\n",
    "\n",
    "### Decisiones de estandarización (base de consulta)\n",
    "1. **No separar nombre/apellido**: se trabaja con el nombre completo.\n",
    "2. Crear `name_norm_strict` como clave estándar para indexación:\n",
    "   - lowercase\n",
    "   - sin acentos\n",
    "   - colapsar espacios\n",
    "   - remover símbolos/dígitos\n",
    "   - remover prefijos de títulos comunes\n",
    "   - preservar `-` y `'`\n",
    "3. Mantener el `Full Name` original para mostrar al usuario (explicabilidad).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e5ec7d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'names_dataset_standardized.csv'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export opcional: dataset estandarizado para inspección / reutilización\n",
    "#out_path = \"./API/data/clean/names_dataset_standardized.csv\"\n",
    "out_path = \"names_dataset_standardized.csv\"\n",
    "df_out = df[[\"ID\", \"Full Name\", \"name_norm_soft\", \"name_norm_strict\"]].copy()\n",
    "df_out.to_csv(out_path, index=False)\n",
    "out_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37612cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Próximo paso\n",
    "- Aplicar la misma normalización al input del usuario (y opcionalmente remover títulos si el usuario los incluye).\n",
    "- Construir el motor de búsqueda (candidate generation + scoring) sobre `name_norm_strict`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e147fd",
   "metadata": {},
   "source": [
    "## Pruebas\n",
    "\n",
    "Vamos a tomar 10 nombres al azar del csv, le generaremos ruido simulando errores humanos, luego ejecutamos el mismo matching que usa la API y comparamos los resultados matcheando contra el dataset original vs el dataset normalizado. Además incluimos una variante estandarizado + dedupe (la que suele mejorar más la performance).\n",
    "\n",
    "Esperamos que mejore:\n",
    "* Performance (latencia/QPS): mejorarás solo si la “base normalizada” te permite comparar contra menos strings (ej: deduplicar por clave normalizada) o evitar cómputo repetido (precomputar name_normalized, tokens, n-grams, etc.).\n",
    "\n",
    "* Calidad (ranking): puede mejorar si la normalización reduce ruido (“Dr.”, símbolos, dobles espacios, acentos, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb359b57",
   "metadata": {},
   "source": [
    "10 nombres al azar + ruido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "674cfe62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_name</th>\n",
       "      <th>query_noisy</th>\n",
       "      <th>query_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ana García González</td>\n",
       "      <td>Ana Garcí González !!</td>\n",
       "      <td>ana garci gonzalez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sofía Jiménez</td>\n",
       "      <td>Sofia Jimnez !!</td>\n",
       "      <td>sofia jimnez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Javier Álvarez Hernández</td>\n",
       "      <td>Javier lvarez Hernandez !!</td>\n",
       "      <td>javier lvarez hernandez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Francisco Fernández</td>\n",
       "      <td>Francisco Fernández</td>\n",
       "      <td>francisco fernandez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dr. Lucía Rodríguez</td>\n",
       "      <td>Dr. Lucía Rodríuez</td>\n",
       "      <td>lucia rodriuez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Pedro García Jiménez</td>\n",
       "      <td>Pedro arcía Jiménez</td>\n",
       "      <td>pedro arcia jimenez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Elena Sánchez Gómez</td>\n",
       "      <td>Elena Sanchz Gomez !!</td>\n",
       "      <td>elena sanchz gomez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sofía Fernández</td>\n",
       "      <td>Sofia ernandez</td>\n",
       "      <td>sofia ernandez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ca@rmen Hernández</td>\n",
       "      <td>Ca@rmen Hernández</td>\n",
       "      <td>ca rmen hernandez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Carmen Hernández Hernández</td>\n",
       "      <td>Carmen Hernández Hrnández !!</td>\n",
       "      <td>carmen hernandez hrnandez</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   clean_name                   query_noisy  \\\n",
       "0         Ana García González         Ana Garcí González !!   \n",
       "1               Sofía Jiménez               Sofia Jimnez !!   \n",
       "2    Javier Álvarez Hernández    Javier lvarez Hernandez !!   \n",
       "3         Francisco Fernández           Francisco Fernández   \n",
       "4         Dr. Lucía Rodríguez            Dr. Lucía Rodríuez   \n",
       "5        Pedro García Jiménez           Pedro arcía Jiménez   \n",
       "6         Elena Sánchez Gómez         Elena Sanchz Gomez !!   \n",
       "7             Sofía Fernández                Sofia ernandez   \n",
       "8           Ca@rmen Hernández             Ca@rmen Hernández   \n",
       "9  Carmen Hernández Hernández  Carmen Hernández Hrnández !!   \n",
       "\n",
       "                  query_norm  \n",
       "0         ana garci gonzalez  \n",
       "1               sofia jimnez  \n",
       "2    javier lvarez hernandez  \n",
       "3        francisco fernandez  \n",
       "4             lucia rodriuez  \n",
       "5        pedro arcia jimenez  \n",
       "6         elena sanchz gomez  \n",
       "7             sofia ernandez  \n",
       "8          ca rmen hernandez  \n",
       "9  carmen hernandez hrnandez  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 10 nombres al azar (reproducible)\n",
    "# -------------------------\n",
    "SAMPLE_SEED = 42\n",
    "sample_names = df[\"Full Name\"].sample(10, random_state=SAMPLE_SEED).tolist()\n",
    "\n",
    "# -------------------------\n",
    "# Simular errores humanos (typos, acentos, símbolos, etc.)\n",
    "# -------------------------\n",
    "random.seed(SAMPLE_SEED)\n",
    "\n",
    "def make_noisy(q: str) -> str:\n",
    "    s = str(q)\n",
    "\n",
    "    # 50%: quitar acentos\n",
    "    if random.random() < 0.5:\n",
    "        s = strip_accents(s)\n",
    "\n",
    "    # 50%: borrar un carácter (typo)\n",
    "    if len(s) > 6 and random.random() < 0.5:\n",
    "        i = random.randint(1, len(s) - 2)\n",
    "        s = s[:i] + s[i+1:]\n",
    "\n",
    "    # 30%: agregar puntuación extra\n",
    "    if random.random() < 0.3:\n",
    "        s = s + \" !!\"\n",
    "\n",
    "    return s\n",
    "\n",
    "queries = []\n",
    "for clean in sample_names:\n",
    "    noisy = make_noisy(clean)\n",
    "    queries.append({\"clean_name\": clean, \"query_noisy\": noisy, \"query_norm\": normalize_strict(noisy)})\n",
    "\n",
    "qdf = pd.DataFrame(queries)\n",
    "qdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32545dc9",
   "metadata": {},
   "source": [
    "Matching (API-like) + benchmark original vs estandarizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0912881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_against_repo(\n",
    "    query: str,\n",
    "    repo_ids: list,\n",
    "    repo_norm_names: list,\n",
    "    threshold: float = 70,\n",
    "    limit: int = 10,\n",
    "    w_token: float = 0.65\n",
    "):\n",
    "    qn = normalize_strict(query)\n",
    "    hits = []\n",
    "\n",
    "    for _id, cand in zip(repo_ids, repo_norm_names):\n",
    "        t = fuzz.token_set_ratio(qn, cand)\n",
    "        e = fuzz.ratio(qn, cand)\n",
    "        s = w_token * t + (1.0 - w_token) * e\n",
    "\n",
    "        if s >= threshold:\n",
    "            hits.append((_id, s, t, e))\n",
    "\n",
    "    # orden determinista (igual que pediste en la API)\n",
    "    hits.sort(key=lambda x: (x[1], x[2], x[3]), reverse=True)\n",
    "\n",
    "    return hits[:limit]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813725f6",
   "metadata": {},
   "source": [
    "3.2 Repos a comparar\n",
    "\n",
    "Repo “original”: usa el CSV original, pero normalizamos en memoria (como hace la API al construir repo)\n",
    "\n",
    "Repo “standardized”: usa name_strict ya precomputado del CSV estandarizado\n",
    "\n",
    "Repo “standardized + dedupe”: elimina duplicados por strict_key (suele mejorar performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9325defa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 5000\n",
      "Unique strict_key: 2717\n",
      "Duplicados por strict_key: 2283\n",
      "Saved: names_dataset_standardized.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "CSV_PATH = \"names_dataset.csv\"\n",
    "\n",
    "# -------------------------\n",
    "# Normalización (misma idea que venimos usando)\n",
    "# -------------------------\n",
    "TITLE_PAT = re.compile(r\"^(dr|dra|sr|sra|srta|ing|lic|prof)\\.?\\s+\", re.IGNORECASE)\n",
    "NON_LETTER = re.compile(r\"[^a-zA-Z\\s]\")\n",
    "MULTISPACE = re.compile(r\"\\s+\")\n",
    "\n",
    "def strip_accents(s: str) -> str:\n",
    "    return \"\".join(c for c in unicodedata.normalize(\"NFKD\", s) if not unicodedata.combining(c))\n",
    "\n",
    "def normalize_strict(name: str) -> str:\n",
    "    s = str(name).strip().lower()\n",
    "    s = strip_accents(s)\n",
    "    s = TITLE_PAT.sub(\"\", s)          # elimina títulos al inicio\n",
    "    s = NON_LETTER.sub(\" \", s)        # elimina símbolos/puntuación\n",
    "    s = MULTISPACE.sub(\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# -------------------------\n",
    "# Cargar dataset\n",
    "# -------------------------\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "assert \"ID\" in df.columns and \"Full Name\" in df.columns\n",
    "\n",
    "df_std = df.copy()\n",
    "df_std[\"name_strict\"] = df_std[\"Full Name\"].map(normalize_strict)\n",
    "\n",
    "# Clave para detectar duplicados semánticos (misma “persona” para el motor)\n",
    "df_std[\"strict_key\"] = df_std[\"name_strict\"]\n",
    "df_std[\"is_strict_dup\"] = df_std.duplicated(\"strict_key\", keep=\"first\")\n",
    "df_std[\"strict_group_size\"] = df_std.groupby(\"strict_key\")[\"ID\"].transform(\"size\")\n",
    "\n",
    "print(\"Rows:\", len(df_std))\n",
    "print(\"Unique strict_key:\", df_std[\"strict_key\"].nunique())\n",
    "print(\"Duplicados por strict_key:\", int(df_std[\"is_strict_dup\"].sum()))\n",
    "\n",
    "# Guardar dataset estandarizado (si querés persistirlo)\n",
    "OUT_STD = \"names_dataset_standardized.csv\"\n",
    "df_std.to_csv(OUT_STD, index=False)\n",
    "print(\"Saved:\", OUT_STD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17427c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repo_original: 5000\n",
      "repo_standardized: 5000\n",
      "repo_dedup: 2717\n"
     ]
    }
   ],
   "source": [
    "ids_all = df_std[\"ID\"].tolist()\n",
    "\n",
    "# A) \"Original\" (normalizo desde Full Name)\n",
    "repo_original_norm = df[\"Full Name\"].astype(str).map(normalize_strict).tolist()\n",
    "\n",
    "# B) \"Standardized\" (ya trae name_strict)\n",
    "repo_standardized_norm = df_std[\"name_strict\"].tolist()\n",
    "\n",
    "# C) \"Standardized + dedupe\" (menos comparaciones)\n",
    "df_dedup = df_std[~df_std[\"is_strict_dup\"]].copy()\n",
    "ids_dedup = df_dedup[\"ID\"].tolist()\n",
    "repo_dedup_norm = df_dedup[\"name_strict\"].tolist()\n",
    "\n",
    "print(\"repo_original:\", len(ids_all))\n",
    "print(\"repo_standardized:\", len(ids_all))\n",
    "print(\"repo_dedup:\", len(ids_dedup))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4679dd84",
   "metadata": {},
   "source": [
    "La eliminación de duplicados en la base a comparar aqui representará una mejora en el benchmark marginal pero en una escala de 50M de datos es sumamente relevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31041f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo</th>\n",
       "      <th>runs</th>\n",
       "      <th>mean_ms</th>\n",
       "      <th>p50_ms</th>\n",
       "      <th>p95_ms</th>\n",
       "      <th>min_ms</th>\n",
       "      <th>max_ms</th>\n",
       "      <th>avg_hits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>original</td>\n",
       "      <td>200</td>\n",
       "      <td>8.218177</td>\n",
       "      <td>7.6870</td>\n",
       "      <td>10.77084</td>\n",
       "      <td>5.8791</td>\n",
       "      <td>35.0053</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>standardized</td>\n",
       "      <td>200</td>\n",
       "      <td>8.570966</td>\n",
       "      <td>8.5748</td>\n",
       "      <td>9.96902</td>\n",
       "      <td>7.2256</td>\n",
       "      <td>14.2805</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>standardized+dedupe</td>\n",
       "      <td>200</td>\n",
       "      <td>5.447388</td>\n",
       "      <td>5.2325</td>\n",
       "      <td>7.55574</td>\n",
       "      <td>4.0855</td>\n",
       "      <td>9.0630</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  repo  runs   mean_ms  p50_ms    p95_ms  min_ms   max_ms  \\\n",
       "0             original   200  8.218177  7.6870  10.77084  5.8791  35.0053   \n",
       "1         standardized   200  8.570966  8.5748   9.96902  7.2256  14.2805   \n",
       "2  standardized+dedupe   200  5.447388  5.2325   7.55574  4.0855   9.0630   \n",
       "\n",
       "   avg_hits  \n",
       "0      10.0  \n",
       "1      10.0  \n",
       "2      10.0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def benchmark(repo_ids, repo_norm_names, queries, runs=30):\n",
    "    lats = []\n",
    "    hitcounts = []\n",
    "\n",
    "    for q in queries:\n",
    "        for _ in range(runs):\n",
    "            t0 = time.perf_counter()\n",
    "            hits = match_against_repo(q, repo_ids, repo_norm_names, threshold=70, limit=10, w_token=0.65)\n",
    "            lats.append((time.perf_counter() - t0) * 1000.0)\n",
    "            hitcounts.append(len(hits))\n",
    "\n",
    "    lats = np.array(lats)\n",
    "    return {\n",
    "        \"runs\": int(len(lats)),\n",
    "        \"mean_ms\": float(lats.mean()),\n",
    "        \"p50_ms\": float(np.percentile(lats, 50)),\n",
    "        \"p95_ms\": float(np.percentile(lats, 95)),\n",
    "        \"min_ms\": float(lats.min()),\n",
    "        \"max_ms\": float(lats.max()),\n",
    "        \"avg_hits\": float(np.mean(hitcounts)),\n",
    "    }\n",
    "\n",
    "queries_noisy = qdf[\"query_noisy\"].tolist()\n",
    "\n",
    "bench_original = benchmark(ids_all, repo_original_norm, queries_noisy, runs=20)\n",
    "bench_standard = benchmark(ids_all, repo_standardized_norm, queries_noisy, runs=20)\n",
    "bench_dedup = benchmark(ids_dedup, repo_dedup_norm, queries_noisy, runs=20)\n",
    "\n",
    "pd.DataFrame([\n",
    "    {\"repo\": \"original\", **bench_original},\n",
    "    {\"repo\": \"standardized\", **bench_standard},\n",
    "    {\"repo\": \"standardized+dedupe\", **bench_dedup},\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9740ec8",
   "metadata": {},
   "source": [
    "3.4 Comparación rápida de resultados (top-5) por query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a168e41f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo</th>\n",
       "      <th>query</th>\n",
       "      <th>rank</th>\n",
       "      <th>id</th>\n",
       "      <th>similarity</th>\n",
       "      <th>token_score</th>\n",
       "      <th>edit_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>original</td>\n",
       "      <td>Ana Garcí González !!</td>\n",
       "      <td>1</td>\n",
       "      <td>1502</td>\n",
       "      <td>97.30</td>\n",
       "      <td>97.30</td>\n",
       "      <td>97.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>original</td>\n",
       "      <td>Ana Garcí González !!</td>\n",
       "      <td>2</td>\n",
       "      <td>1022</td>\n",
       "      <td>93.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>80.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>original</td>\n",
       "      <td>Ana Garcí González !!</td>\n",
       "      <td>3</td>\n",
       "      <td>1172</td>\n",
       "      <td>93.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>80.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>original</td>\n",
       "      <td>Ana Garcí González !!</td>\n",
       "      <td>4</td>\n",
       "      <td>1594</td>\n",
       "      <td>93.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>80.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>original</td>\n",
       "      <td>Ana Garcí González !!</td>\n",
       "      <td>5</td>\n",
       "      <td>2277</td>\n",
       "      <td>93.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>80.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>standardized</td>\n",
       "      <td>Ana Garcí González !!</td>\n",
       "      <td>1</td>\n",
       "      <td>1502</td>\n",
       "      <td>97.30</td>\n",
       "      <td>97.30</td>\n",
       "      <td>97.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>standardized</td>\n",
       "      <td>Ana Garcí González !!</td>\n",
       "      <td>2</td>\n",
       "      <td>1022</td>\n",
       "      <td>93.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>80.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>standardized</td>\n",
       "      <td>Ana Garcí González !!</td>\n",
       "      <td>3</td>\n",
       "      <td>1172</td>\n",
       "      <td>93.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>80.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>standardized</td>\n",
       "      <td>Ana Garcí González !!</td>\n",
       "      <td>4</td>\n",
       "      <td>1594</td>\n",
       "      <td>93.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>80.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>standardized</td>\n",
       "      <td>Ana Garcí González !!</td>\n",
       "      <td>5</td>\n",
       "      <td>2277</td>\n",
       "      <td>93.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>80.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>standardized+dedupe</td>\n",
       "      <td>Ana Garcí González !!</td>\n",
       "      <td>1</td>\n",
       "      <td>1502</td>\n",
       "      <td>97.30</td>\n",
       "      <td>97.30</td>\n",
       "      <td>97.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>standardized+dedupe</td>\n",
       "      <td>Ana Garcí González !!</td>\n",
       "      <td>2</td>\n",
       "      <td>1022</td>\n",
       "      <td>93.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>80.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>standardized+dedupe</td>\n",
       "      <td>Ana Garcí González !!</td>\n",
       "      <td>3</td>\n",
       "      <td>3666</td>\n",
       "      <td>91.92</td>\n",
       "      <td>100.00</td>\n",
       "      <td>76.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>standardized+dedupe</td>\n",
       "      <td>Ana Garcí González !!</td>\n",
       "      <td>4</td>\n",
       "      <td>3781</td>\n",
       "      <td>87.18</td>\n",
       "      <td>87.18</td>\n",
       "      <td>87.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>standardized+dedupe</td>\n",
       "      <td>Ana Garcí González !!</td>\n",
       "      <td>5</td>\n",
       "      <td>405</td>\n",
       "      <td>86.49</td>\n",
       "      <td>86.49</td>\n",
       "      <td>86.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>original</td>\n",
       "      <td>Ca@rmen Hernández</td>\n",
       "      <td>1</td>\n",
       "      <td>2414</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>original</td>\n",
       "      <td>Ca@rmen Hernández</td>\n",
       "      <td>2</td>\n",
       "      <td>404</td>\n",
       "      <td>96.97</td>\n",
       "      <td>96.97</td>\n",
       "      <td>96.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>original</td>\n",
       "      <td>Ca@rmen Hernández</td>\n",
       "      <td>3</td>\n",
       "      <td>829</td>\n",
       "      <td>96.97</td>\n",
       "      <td>96.97</td>\n",
       "      <td>96.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>original</td>\n",
       "      <td>Ca@rmen Hernández</td>\n",
       "      <td>4</td>\n",
       "      <td>1993</td>\n",
       "      <td>96.97</td>\n",
       "      <td>96.97</td>\n",
       "      <td>96.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>original</td>\n",
       "      <td>Ca@rmen Hernández</td>\n",
       "      <td>5</td>\n",
       "      <td>2403</td>\n",
       "      <td>96.97</td>\n",
       "      <td>96.97</td>\n",
       "      <td>96.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>standardized</td>\n",
       "      <td>Ca@rmen Hernández</td>\n",
       "      <td>1</td>\n",
       "      <td>2414</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>standardized</td>\n",
       "      <td>Ca@rmen Hernández</td>\n",
       "      <td>2</td>\n",
       "      <td>404</td>\n",
       "      <td>96.97</td>\n",
       "      <td>96.97</td>\n",
       "      <td>96.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>standardized</td>\n",
       "      <td>Ca@rmen Hernández</td>\n",
       "      <td>3</td>\n",
       "      <td>829</td>\n",
       "      <td>96.97</td>\n",
       "      <td>96.97</td>\n",
       "      <td>96.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>standardized</td>\n",
       "      <td>Ca@rmen Hernández</td>\n",
       "      <td>4</td>\n",
       "      <td>1993</td>\n",
       "      <td>96.97</td>\n",
       "      <td>96.97</td>\n",
       "      <td>96.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>standardized</td>\n",
       "      <td>Ca@rmen Hernández</td>\n",
       "      <td>5</td>\n",
       "      <td>2403</td>\n",
       "      <td>96.97</td>\n",
       "      <td>96.97</td>\n",
       "      <td>96.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>standardized+dedupe</td>\n",
       "      <td>Ca@rmen Hernández</td>\n",
       "      <td>1</td>\n",
       "      <td>2414</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>standardized+dedupe</td>\n",
       "      <td>Ca@rmen Hernández</td>\n",
       "      <td>2</td>\n",
       "      <td>404</td>\n",
       "      <td>96.97</td>\n",
       "      <td>96.97</td>\n",
       "      <td>96.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>standardized+dedupe</td>\n",
       "      <td>Ca@rmen Hernández</td>\n",
       "      <td>3</td>\n",
       "      <td>1601</td>\n",
       "      <td>89.08</td>\n",
       "      <td>96.97</td>\n",
       "      <td>74.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>standardized+dedupe</td>\n",
       "      <td>Ca@rmen Hernández</td>\n",
       "      <td>4</td>\n",
       "      <td>1584</td>\n",
       "      <td>88.89</td>\n",
       "      <td>88.89</td>\n",
       "      <td>88.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>standardized+dedupe</td>\n",
       "      <td>Ca@rmen Hernández</td>\n",
       "      <td>5</td>\n",
       "      <td>4748</td>\n",
       "      <td>86.49</td>\n",
       "      <td>86.49</td>\n",
       "      <td>86.49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    repo                  query  rank    id  similarity  \\\n",
       "0               original  Ana Garcí González !!     1  1502       97.30   \n",
       "1               original  Ana Garcí González !!     2  1022       93.00   \n",
       "2               original  Ana Garcí González !!     3  1172       93.00   \n",
       "3               original  Ana Garcí González !!     4  1594       93.00   \n",
       "4               original  Ana Garcí González !!     5  2277       93.00   \n",
       "50          standardized  Ana Garcí González !!     1  1502       97.30   \n",
       "51          standardized  Ana Garcí González !!     2  1022       93.00   \n",
       "52          standardized  Ana Garcí González !!     3  1172       93.00   \n",
       "53          standardized  Ana Garcí González !!     4  1594       93.00   \n",
       "54          standardized  Ana Garcí González !!     5  2277       93.00   \n",
       "100  standardized+dedupe  Ana Garcí González !!     1  1502       97.30   \n",
       "101  standardized+dedupe  Ana Garcí González !!     2  1022       93.00   \n",
       "102  standardized+dedupe  Ana Garcí González !!     3  3666       91.92   \n",
       "103  standardized+dedupe  Ana Garcí González !!     4  3781       87.18   \n",
       "104  standardized+dedupe  Ana Garcí González !!     5   405       86.49   \n",
       "40              original      Ca@rmen Hernández     1  2414      100.00   \n",
       "41              original      Ca@rmen Hernández     2   404       96.97   \n",
       "42              original      Ca@rmen Hernández     3   829       96.97   \n",
       "43              original      Ca@rmen Hernández     4  1993       96.97   \n",
       "44              original      Ca@rmen Hernández     5  2403       96.97   \n",
       "90          standardized      Ca@rmen Hernández     1  2414      100.00   \n",
       "91          standardized      Ca@rmen Hernández     2   404       96.97   \n",
       "92          standardized      Ca@rmen Hernández     3   829       96.97   \n",
       "93          standardized      Ca@rmen Hernández     4  1993       96.97   \n",
       "94          standardized      Ca@rmen Hernández     5  2403       96.97   \n",
       "140  standardized+dedupe      Ca@rmen Hernández     1  2414      100.00   \n",
       "141  standardized+dedupe      Ca@rmen Hernández     2   404       96.97   \n",
       "142  standardized+dedupe      Ca@rmen Hernández     3  1601       89.08   \n",
       "143  standardized+dedupe      Ca@rmen Hernández     4  1584       88.89   \n",
       "144  standardized+dedupe      Ca@rmen Hernández     5  4748       86.49   \n",
       "\n",
       "     token_score  edit_score  \n",
       "0          97.30       97.30  \n",
       "1         100.00       80.00  \n",
       "2         100.00       80.00  \n",
       "3         100.00       80.00  \n",
       "4         100.00       80.00  \n",
       "50         97.30       97.30  \n",
       "51        100.00       80.00  \n",
       "52        100.00       80.00  \n",
       "53        100.00       80.00  \n",
       "54        100.00       80.00  \n",
       "100        97.30       97.30  \n",
       "101       100.00       80.00  \n",
       "102       100.00       76.92  \n",
       "103        87.18       87.18  \n",
       "104        86.49       86.49  \n",
       "40        100.00      100.00  \n",
       "41         96.97       96.97  \n",
       "42         96.97       96.97  \n",
       "43         96.97       96.97  \n",
       "44         96.97       96.97  \n",
       "90        100.00      100.00  \n",
       "91         96.97       96.97  \n",
       "92         96.97       96.97  \n",
       "93         96.97       96.97  \n",
       "94         96.97       96.97  \n",
       "140       100.00      100.00  \n",
       "141        96.97       96.97  \n",
       "142        96.97       74.42  \n",
       "143        88.89       88.89  \n",
       "144        86.49       86.49  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def topk_table(repo_name, repo_ids, repo_norm_names):\n",
    "    rows = []\n",
    "    for q in queries_noisy:\n",
    "        hits = match_against_repo(q, repo_ids, repo_norm_names, threshold=70, limit=5, w_token=0.65)\n",
    "        for rank, (id_, s, t, e) in enumerate(hits, start=1):\n",
    "            rows.append({\n",
    "                \"repo\": repo_name,\n",
    "                \"query\": q,\n",
    "                \"rank\": rank,\n",
    "                \"id\": int(id_),\n",
    "                \"similarity\": round(s, 2),\n",
    "                \"token_score\": round(t, 2),\n",
    "                \"edit_score\": round(e, 2),\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "out = pd.concat([\n",
    "    topk_table(\"original\", ids_all, repo_original_norm),\n",
    "    topk_table(\"standardized\", ids_all, repo_standardized_norm),\n",
    "    topk_table(\"standardized+dedupe\", ids_dedup, repo_dedup_norm),\n",
    "], ignore_index=True)\n",
    "\n",
    "out.sort_values([\"query\", \"repo\", \"rank\"]).head(30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69db8118",
   "metadata": {},
   "source": [
    "Acá vemos que en el primer caso en la comparación original vs standarized la estandarizacion no cambia el ranquing, estamos normalizando la query antes de la consulta y el dataset no tiene un ruido estructural grave por lo que el cambio no es sustancial. En algunos casos como el 2 y 3 como antes había muchos registros casi identicos ahora al eliminar los duplicados semánticos se liberan slots del top-k por lo que aparecen candidatos distintos pero razonables, no es que empeora el modelo sino que compara contra más variedad. Esto es deseable en producción donde se hidrata por ID y se aplica un re-ranking posterior. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14776459",
   "metadata": {},
   "source": [
    "Con respecto al benchmark, la estandarización comun no solo no reduce la latencia sino que es marginalmente peor por ruido estadístico. Sin embargo la deduplicación semántica si genera una mejora sustancial 35% menos de latencia media y un p95 mucho mas estable con menos picos máximos de latencia. Lo cual es sustancialmente mejor porque genera menos candidatos reales, menos comparaciones, menor trabajo por query y mejor predictivilidad. Esto claramente será preferible con 50 millones de datos.\n",
    "En este sentido, la estandarización se justifica no como un mecanismo de mejora directa del matching, sino como un habilitador clave para escalar el sistema de forma eficiente y predecible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40265b70",
   "metadata": {},
   "source": [
    "En producción lo que deberíamos hacer es:\n",
    "\n",
    "✔ Mantener:\n",
    "\n",
    "+ Source of Truth con datos originales\n",
    "\n",
    "+ IDs intactos\n",
    "\n",
    "✔ Construir:\n",
    "\n",
    "+ tabla/índice de búsqueda estandarizado\n",
    "\n",
    "+ clave normalizada (strict_key)\n",
    "\n",
    "+ estructura deduplicada para candidate retrieval\n",
    "\n",
    "✔ En la API:\n",
    "\n",
    "+ buscar contra el índice deduplicado\n",
    "\n",
    "+ devolver IDs\n",
    "\n",
    "+ hidratar desde SoT\n",
    "\n",
    "+ opcionalmente expandir a todos los IDs del grupo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c813d9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_api",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
